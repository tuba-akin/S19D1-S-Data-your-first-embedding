{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ä°lk embeddingâ€™iniz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Egzersiz hedefleri:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLP iÃ§in ilk RNN'nizi Ã§alÄ±ÅŸtÄ±rÄ±n\n",
    "- GÃ¶mme kavramÄ±nÄ±n ne olduÄŸunu ilk kez deneyimleyin\n",
    "\n",
    "<hr>\n",
    "\n",
    "Kelimeler, sinir aÄŸÄ±na kolayca besleyebileceÄŸiniz ÅŸeyler deÄŸildir. Bu nedenle, onlarÄ± daha anlamlÄ± bir ÅŸeye dÃ¶nÃ¼ÅŸtÃ¼rmemiz gerekir. \n",
    "\n",
    "Ve _Embeddings_ kavramÄ± tam da bunun iÃ§in vardÄ±r! Herhangi bir kelimeyi vektÃ¶rel bir temsil Ã¼zerine eÅŸler (bu, her kelimeyi bir vektÃ¶rle temsil etmenin sÃ¼slÃ¼ bir yoludur ;) ). Ã–rneÄŸin, `kÃ¶pek` kelimesi gÃ¶mme uzayÄ±nda $(w_1, w_2, ..., w_n)$ vektÃ¶rÃ¼ ile temsil edilebilir ve biz de $(w_k)_k$ aÄŸÄ±rlÄ±klarÄ±nÄ± Ã¶ÄŸreniriz.\n",
    "\n",
    "Ã–yleyse haydi baÅŸlayalÄ±m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Soru** â“ Ã–ncelikle verileri yÃ¼kleyelim. Fonksiyonda neler olduÄŸunu anlamanÄ±za gerek yok, burada Ã¶nemi yok.\n",
    "\n",
    "âš ï¸ **UyarÄ±** âš ï¸ `load_data` fonksiyonunun bir `percentage_of_sentences` argÃ¼manÄ± vardÄ±r. BilgisayarÄ±nÄ±za baÄŸlÄ± olarak, Ã§ok fazla cÃ¼mle bilgisayarÄ±nÄ±zÄ± yavaÅŸlatabilir veya hatta dondurabilir - RAM'iniz taÅŸabilir. Bu nedenle, **cÃ¼mlelerin %10'uyla baÅŸlamalÄ±** ve bilgisayarÄ±nÄ±zÄ±n bunu kaldÄ±rabildiÄŸini gÃ¶rmelisiniz. Aksi takdirde, daha dÃ¼ÅŸÃ¼k bir sayÄ± ile yeniden Ã§alÄ±ÅŸtÄ±rÄ±n. \n",
    "\n",
    "âš ï¸ **DISCLAIMER** âš ï¸ **_En bÃ¼yÃ¼ÄŸÃ¼ kimde_ (RAM) oyununu oynamaya gerek yok!** Buradaki amaÃ§, modellerinizi hÄ±zlÄ± bir ÅŸekilde Ã§alÄ±ÅŸtÄ±rarak prototip oluÅŸturmaktÄ±r. GerÃ§ek hayatta bile, hÄ±zlÄ± bir ÅŸekilde dÃ¶ngÃ¼ ve hata ayÄ±klama yapmak iÃ§in verilerinizin bir alt kÃ¼mesiyle baÅŸlamanÄ±z Ã¶nerilir. Bu nedenle, yalnÄ±zca en iyi doÄŸruluÄŸu elde etmek istiyorsanÄ±z sayÄ±yÄ± artÄ±rÄ±n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "### Verileri yÃ¼klemek iÃ§in bu hÃ¼creyi Ã§alÄ±ÅŸtÄ±rÄ±n  ###\n",
    "#####################################################\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "\n",
    "    # TÃ¼m verilerin yalnÄ±zca belirli bir yÃ¼zdesini alÄ±n\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "\n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "\n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "\n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verileri yÃ¼klediÄŸinize gÃ¶re, ÅŸimdi bir gÃ¶z atalÄ±m!\n",
    "\n",
    "â“ **Soru** â“ Burada verilerle oynayabilirsiniz. Ã–zellikle, `X_train` ve `X_test` cÃ¼mle listeleridir. Bunlardan bazÄ±larÄ±nÄ±, `y_train` ve `y_test` iÃ§inde saklanan ilgili etiketleriyle birlikte yazdÄ±rÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# SENÄ°N KODUN BURAYA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ETÄ°KETLER**: GÃ¶rev bir ikili sÄ±nÄ±flandÄ±rma problemidir:\n",
    "- etiket 0ï¸âƒ£ <font color=red>negatif</font> film yorumuna karÅŸÄ±lÄ±k gelir\n",
    "- etiket 1ï¸âƒ£ <font color=green>pozitif</font> film yorumuna karÅŸÄ±lÄ±k gelir\n",
    "\n",
    "**GÄ°RDÄ°LER**:\n",
    "- ğŸ§¹ Veri kÄ±smen temizlenmiÅŸtir! Bu yÃ¼zden bu egzersizde bununla ilgilenmenize gerek yok.\n",
    "- â—ï¸ Ancak gerÃ§ek hayattaki Ã§alÄ±ÅŸmalarda bu adÄ±mÄ± unutmamalÄ±sÄ±nÄ±z.\n",
    "\n",
    "Kelimelerin bilgisayarlar iÃ§in doÄŸrudan uygun olmadÄ±ÄŸÄ±nÄ± hatÄ±rlÄ±yor musunuz?  \n",
    "OnlarÄ± **tokenize** etmeniz gerekir!\n",
    "\n",
    "â“ **Soru** â“ AÅŸaÄŸÄ±daki hÃ¼creyi Ã§alÄ±ÅŸtÄ±rarak cÃ¼mlelerinizi tokenize edin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Bu, tÃ¼m tokenizasyonu sizin iÃ§in yapan bir Keras yardÄ±mcÄ± programÄ±nÄ± baÅŸlatÄ±r.\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Tokenizasyon, bir tokeni (integer) her kelimeye eÅŸleyen bir sÃ¶zlÃ¼ÄŸÃ¼ Ã¶ÄŸrenir.\n",
    "# Bu, yalnÄ±zca eÄŸitim setinde yapÄ±labilir - test setini bilmemiz gerekmez!\n",
    "# Bu tokenizasyon ayrÄ±ca kelimelerinizi kÃ¼Ã§Ã¼k harfe Ã§evirir, bazÄ± filtreler uygular vb. - isterseniz dokÃ¼manÄ± kontrol edebilirsiniz.\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Tokenizasyonu eÄŸitim ve test setine uygularÄ±z.\n",
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Soru** â“ BeklediÄŸiniz sonucu aldÄ±ÄŸÄ±nÄ±zdan emin olmak iÃ§in tokenize edilmiÅŸ cÃ¼mlelerin bazÄ±larÄ±nÄ± yazdÄ±rÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# SENÄ°N KODUN BURAYA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her kelimeyi bir token ile eÅŸleÅŸtiren sÃ¶zlÃ¼ÄŸe `tokenizer.word_index` ile eriÅŸilebilir.\n",
    "\n",
    "â“ **Soru** â“ EÄŸitim setindeki farklÄ± kelimelerin (=tokenlerin) sayÄ±sÄ±nÄ± depolayan bir `vocab_size` deÄŸiÅŸkeni ekleyin. Buna _sÃ¶zcÃ¼k daÄŸarcÄ±ÄŸÄ±nÄ±n boyutu_ (_size of the vocabulary_) denir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# SENÄ°N KODUN BURAYA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_train_token` ve `X_test_token` farklÄ± uzunluklarda diziler iÃ§erir.\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/06-DL/NLP/padding.png\" alt='Padding example' width=\"700px\" />\n",
    "\n",
    "Ancak, bir sinir aÄŸÄ± girdi olarak bir tensÃ¶r iÃ§ermelidir. Bu nedenle, verilerinizi doldurmanÄ±z gerekir.\n",
    "\n",
    "â“ **Soru** â“  Verilerinizi `pad_sequences` iÅŸleviyle doldurun (belgeler [burada](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)). `dtype` ve `padding` anahtar kelimelerini unutmayÄ±n (ancak burada `maxlen` kullanmayÄ±n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# SENÄ°N KODUN BURAYA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Åimdi bu verileri Tekrarlayan Sinir AÄŸÄ±na aktaralÄ±m.\n",
    "\n",
    "â“ **Soru** â“ AÅŸaÄŸÄ±dakileri iÃ§eren bir model yazÄ±n:\n",
    "- `input_dim` deÄŸeri kelime daÄŸarcÄ±ÄŸÄ±nÄ±zÄ±n boyutu (= `vocab_size`) olan ve `output_dim` deÄŸeri istediÄŸiniz gÃ¶mme alanÄ±nÄ±n boyutu olan bir gÃ¶mme katmanÄ±\n",
    "- bir RNN (SimpleRNN, LSTM, GRU) katmanÄ±\n",
    "- bir YoÄŸun katman\n",
    "- bir Ã§Ä±ktÄ± katmanÄ±\n",
    "\n",
    "âš ï¸ **UyarÄ±** âš ï¸ Burada maskeleme katmanÄ±na ihtiyacÄ±nÄ±z yoktur. Neden? Ã‡Ã¼nkÃ¼ `layers.Embedding` bunu doÄŸrudan yapmak iÃ§in bir argÃ¼mana sahiptir ve bunu `mask_zero=True` ile ayarlamanÄ±z gerekir. Bu aynÄ± zamanda verilerinizin **0** ile doldurulmasÄ± **GEREKTÄ°ÄÄ°** anlamÄ±na gelir (bu varsayÄ±lan davranÄ±ÅŸtÄ±r). Bunun `input_dim`'i nasÄ±l **etkilediÄŸini** anlamak iÃ§in [belgelere](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding#example_2) bakÄ±n.\n",
    "\n",
    "<details>\n",
    "    <summary>ğŸ’¡Ä°pucu</summary>\n",
    "\n",
    "`input_dim` kelime daÄŸarcÄ±ÄŸÄ±nÄ±n bÃ¼yÃ¼klÃ¼ÄŸÃ¼ + 1'e eÅŸit olmalÄ±dÄ±r\n",
    "\n",
    "</details>\n",
    "\n",
    "Uygun argÃ¼manlarla derleyin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# SENÄ°N KODUN BURAYA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Soru** â“ RNN'nizdeki parametre sayÄ±sÄ±na bakÄ±n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# SENÄ°N KODUN BURAYA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ GÃ¶mme katmanÄ±nÄ±zdaki parametre sayÄ±sÄ±nÄ±n (kelime daÄŸarcÄ±ÄŸÄ±nÄ±zdaki kelime sayÄ±sÄ± + maskeleme deÄŸeri iÃ§in 1) $\\times$ gÃ¶mme boyutunuzla eÅŸit olduÄŸunu iki kez kontrol edin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# SENÄ°N KODUN BURAYA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Soru** â“ Modelinizi 20 dÃ¶nemle ve sabÄ±r deÄŸeri 4 olan erken durdurma kriteriyle uyumlaÅŸtÄ±rmaya baÅŸlayÄ±n.\n",
    "\n",
    "âš ï¸ **UyarÄ±** âš ï¸ Bunun Ã§ok zaman aldÄ±ÄŸÄ±nÄ± gÃ¶rebilirsiniz! \n",
    "\n",
    "**Bu yÃ¼zden birkaÃ§ yinelemeden sonra durdurun!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# SENÄ°N KODUN BURAYA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ekrana bakarak veya kahve iÃ§erek Ã§ok fazla zaman kaybetmeyelim. Ara vermeye baÅŸlamak iÃ§in henÃ¼z Ã§ok erken ;)\n",
    "\n",
    "â“ **Soru** â“ Hesaplama sÃ¼resini kÄ±saltacaÄŸÄ±z. BaÅŸlangÄ±Ã§ olarak, tren setinizin farklÄ± cÃ¼mlelerinde kaÃ§ kelime olduÄŸunu gÃ¶relim (aÅŸaÄŸÄ±daki hÃ¼creyi Ã§alÄ±ÅŸtÄ±rÄ±n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_hist(X):\n",
    "    len_ = [len(_) for _ in X]\n",
    "    plt.hist(len_)\n",
    "    plt.title('Histogram of the number of sentences that have a given number of words')\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muhtemelen cÃ¼mlelerinizin %90 ila %95'inin 300 kelimeden az olduÄŸunu gÃ¶receksiniz. Ve Ã§ok azÄ± 1000 kelimeden fazladÄ±r.\n",
    "\n",
    "Ancak, yukarÄ±daki dolguda `maxlen` kullanmadÄ±ÄŸÄ±nÄ±z iÃ§in, girdi tensÃ¶rÃ¼nÃ¼zÃ¼n boyutu, en fazla kelimeye sahip cÃ¼mlenin uzunluÄŸuna eÅŸittir.\n",
    "\n",
    "Åimdi, bunun dolguyu nasÄ±l etkilediÄŸine bakalÄ±m: \n",
    "\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/06-DL/NLP/tensor_size.png\" alt='Dimensions of the tensor' width=\"700px\" />\n",
    "\n",
    "BirkaÃ§ Ã§ok uzun cÃ¼mle nedeniyle, tensÃ¶rÃ¼nÃ¼zÃ¼n bir boyutu yaklaÅŸÄ±k 1000'e eÅŸittir. Ancak, ~200 kelimelik cÃ¼mlelerin Ã§oÄŸu, iÅŸe yaramayan dolgu deÄŸerlerine sahiptir.\n",
    "\n",
    "DolayÄ±sÄ±yla, tensÃ¶rÃ¼nÃ¼z Ã§oÄŸunlukla iÅŸe yaramayan bilgilerden oluÅŸur ve bu da eÄŸitim sÃ¼recine zaman ekler.\n",
    "\n",
    "Peki, verileri maksimum uzunluk (`maxlen`) olarak Ã¶rneÄŸin 200 (kelime) olacak ÅŸekilde doldurursanÄ±z ne olur?\n",
    "- Ä°lk olarak, bu yakÄ±nsamayÄ± artÄ±racak ve algoritmanÄ±n yakÄ±nsamayÄ± beklerken ekrana bakmanÄ±za gerek kalmayacaktÄ±r.\n",
    "- Ancak esasen, gerÃ§ekten bu kadar Ã§ok bilgiyi kaybediyor musunuz? Bir cÃ¼mlenin olumlu mu olumsuz mu olduÄŸunu anlamak iÃ§in genellikle 200 kelimeden (1000'e kadar) fazlasÄ±na ihtiyaÃ§ duyduÄŸunuzu dÃ¼ÅŸÃ¼nÃ¼yor musunuz?\n",
    "\n",
    "â“ **Soru** â“ TÃ¼m bu nedenlerden dolayÄ±, `maxlen` anahtar kelimesini kullanarak dolguyu yeniden yapÄ±n ve modeli yeniden eÄŸitin!  PerformansÄ± dÃ¼ÅŸÃ¼rmeden, ÅŸimdi ne kadar hÄ±zlÄ± olduÄŸunu gÃ¶rÃ¼n ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# SENÄ°N KODUN BURAYA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ Harika, artÄ±k `Tokenizer` ve `pad_sequences` kullanabilirsiniz.\n",
    "\n",
    "ğŸ’¾ Not defterinizi git add/commit/push yapmayÄ± unutmayÄ±n...\n",
    "\n",
    "ğŸš€ ... ve bir sonraki gÃ¶reve geÃ§in!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
